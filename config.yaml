# AI Podcast Creator Configuration

# Application Settings
app:
  name: "AI Podcast Creator"
  version: "1.0.0"
  debug: false

# Character Settings
character:
  name: "Vivienne Sterling"
  voice_type: "British Female"
  accent: "Received Pronunciation"
  personality: "Professional, warm, engaging"

# TTS Configuration
# NOTE: Coqui TTS requires Python < 3.12. For Python 3.13+, use gtts or edge-tts
tts:
  engine: "gtts"  # Options: gtts (cloud, works with Python 3.13), edge (cloud, Python 3.13), coqui (GPU, requires Python < 3.12), elevenlabs, azure, piper
  
  # Coqui TTS Settings (default)
  coqui:
    model: "tts_models/multilingual/multi-dataset/xtts_v2"
    language: "en"
    speaker_wav: null  # Path to reference voice (optional)
    temperature: 0.7
    
  # ElevenLabs Settings (if using)
  elevenlabs:
    api_key: "${ELEVENLABS_API_KEY}"
    voice_id: "EXAVITQu4vr4xnSDxMaL"  # Sarah (British)
    model: "eleven_multilingual_v2"
    stability: 0.5
    similarity_boost: 0.75
    
  # Azure Speech Settings (if using)
  azure:
    api_key: "${AZURE_SPEECH_KEY}"
    region: "${AZURE_REGION}"
    voice_name: "en-GB-SoniaNeural"
    
  # Common TTS Settings
  sample_rate: 24000
  output_format: "wav"

# Music Generation
music:
  engine: "musicgen"  # Options: musicgen, mubert, library
  
  musicgen:
    model: "facebook/musicgen-medium"  # Options: small, medium, large
    duration: 10  # seconds per generation
    temperature: 1.0
    top_k: 250
    top_p: 0.0
    
  # Auto-ducking (lower music volume when voice plays)
  ducking:
    enabled: true
    voice_volume: 1.0
    music_volume_during_speech: 0.2
    music_volume_no_speech: 0.5
    fade_duration: 0.5  # seconds

# Avatar/Talking Head
avatar:
  engine: "wav2lip"  # Options: wav2lip (working, recommended), sadtalker (broken: NumPy 2.x incompatibility), did
  
  sadtalker:
    checkpoint_dir: "./data/models/sadtalker"
    config_dir: "./data/models/sadtalker/config"
    enhancer: "gfpgan"  # Options: gfpgan, none
    still_mode: false
    expression_scale: 1.0
    
  wav2lip:
    checkpoint_path: "./data/models/wav2lip/wav2lip_gan.pth"
    face_detect_model: "s3fd"
    
  # D-ID API Settings (if using)
  did:
    api_key: "${DID_API_KEY}"
    presenter_id: "amy-jcwCkr1grs"
    
  # Avatar Image
  source_image: "Creations/MMedia/JE_Static_Image.jpg"
  
# Video Settings
video:
  resolution: [1920, 1080]  # Width x Height
  fps: 30
  codec: "libx264"
  profile: "baseline"  # baseline = max compatibility, main/high = better quality
  level: "3.0"  # Compatibility level
  pix_fmt: "yuv420p"  # Standard pixel format
  bitrate: "2000k"  # Reasonable bitrate
  preset: "medium"  # Options: ultrafast, fast, medium, slow
  audio_codec: "aac"  # AAC for compatibility
  audio_bitrate: "128k"
  
  # Background
  background_type: "image"  # Options: image, video, generated
  background_path: "./src/assets/backgrounds/studio_01.jpg"
  
  # Composition
  avatar_position: [640, 140]  # X, Y offset
  avatar_scale: 1.0
  
  # Effects
  add_subtitles: false
  add_watermark: false
  watermark_text: "Created with AI Podcast Creator"

# Audio Visualization (use --visualize flag)
visualization:
  style: "waveform"  # Options: waveform, spectrum, circular, particles
  primary_color: [0, 255, 0]  # RGB - Neon Green (bright electric green)
  secondary_color: [0, 255, 100]  # RGB - Neon Green-Cyan (bright lime green)
  background_color: [0, 0, 0]  # RGB - Pure black for chromakey transparency
  blur: 0  # Glow effect intensity (0-10) - Set to 0 for completely solid lines (no blur = no grain)
  sensitivity: 1.0  # Audio reactivity (0.5-2.0) - Waveform uses 3x internal multiplier
  
  # Waveform Configuration (NEW - comprehensive controls)
  waveform:
    # Quality & Smoothness (FIXES GRAININESS)
    render_scale: 2.0  # Render at 2x resolution, scale down (smooths pixelation)
    anti_alias: true   # Enable OpenCV anti-aliasing for smooth lines
    
    # Position & Layout
    position: "bottom"  # top, bottom, left, right, middle, or "top,bottom", "left,right", "top,left,right"
    orientation: "auto"  # auto, horizontal, vertical (auto: horizontal for top/bottom, vertical for left/right)
    height_percent: 25  # For horizontal waveforms (10-50% of screen height)
    width_percent: 25   # For vertical waveforms (10-50% of screen width)
    
    # Line Configuration
    num_lines: 1  # Single solid line for bottom_default
    line_thickness: 25  # Thicker line for better visibility (25 pixels minimum)
    line_colors:  # Per-line colors (array of RGB), or null to use primary_color
      - [0, 255, 0]      # Neon green (line 1)
      - [0, 255, 100]    # Lime green (line 2)
      - [0, 200, 50]     # Darker green (line 3)
    
    # Spacing Control (for vertical waveforms)
    left_spacing: 0   # Pixels from left edge for left-side waveform
    right_spacing: 0  # Pixels from right edge for right-side waveform
    
    # Advanced Features
    opacity: 1.0  # 0.0-1.0 (transparency)
    blend_mode: "normal"  # normal, screen, add, overlay (currently not used in chromakey pipeline)
    waveform_style: "continuous"  # continuous, bars, dots, filled
    randomize: false  # Randomize all settings per video (for variety)
    
    # New Advanced Features
    orientation_offset: null  # 0-100 for horizontal waveforms (0=bottom, 100=top), null = use position
    rotation: 0  # Rotation angle in degrees (0 = no rotation)
    amplitude_multiplier: 1.0  # Multiplier for wave amplitude (beyond sensitivity)
    num_instances: 1  # Number of waveform instances
    instances_offset: 0  # Spacing between instances in pixels
    instances_intersect: false  # Allow instances to intersect

# Avatar / Talking Head (use --avatar flag) - DUPLICATE SECTION (removed)
# Note: Avatar configuration is defined above at line 65
# This section was overriding the source_image setting above
  wav2lip:
    model: "wav2lip_gan"  # Model type
    quality: "high"  # Options: high, fast
    face_detection: "blazeface"  # Face detector
  sadtalker:
    checkpoint_dir: "models/sadtalker"
    config_dir: "models/sadtalker/config"
    enhancer: "gfpgan"  # Face enhancer
    still_mode: false  # True = less head movement
    expression_scale: 1.0  # Expression intensity
  did:
    api_key: ""  # D-ID API key (commercial)

# Storage
storage:
  base_dir: "./data"
  scripts_dir: "./Creations/Scripts"  # Input script files
  outputs_dir: "./Creations/MMedia"   # Multimedia output (videos, audio)
  cache_dir: "./data/cache"           # Internal cache (TTS, music generation)
  models_dir: "./data/models"         # Model files (SADTalker, Wav2Lip, etc.)
  
  # Cleanup old cache files
  auto_cleanup: true
  cache_retention_days: 7

# Processing
processing:
  # Use GPU if available (automatically detected)
  use_gpu: true
  gpu_device: 0  # GPU index (0 for first GPU, 1 for second, etc.)
  
  # GPU Optimizations (auto-enabled based on GPU capability)
  enable_fp16: true  # Mixed precision (requires Tensor Cores - RTX/V100+)
  enable_tf32: true  # TF32 acceleration (Ampere GPUs - RTX 30/40 series)
  enable_cudnn_benchmark: true  # cuDNN auto-tuning for optimal performance
  enable_torch_compile: true  # PyTorch 2.0+ compilation (significant speedup)
  
  # Memory Management
  clear_cache_between_steps: true  # Clear GPU cache between operations
  max_memory_allocated: 0.9  # Use up to 90% of available VRAM
  
  # Async processing
  async_mode: false  # Enable for Celery task queue
  
  # Performance
  batch_size: 1  # Auto-adjusted based on available VRAM
  num_workers: 4  # Auto-adjusted based on CPU cores
  
# Database
database:
  url: "sqlite:///./data/podcasts.db"
  echo: false  # SQL logging

# Logging
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  format: "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>"
  file: "./logs/app.log"
  rotation: "10 MB"
  retention: "1 week"

# API (Optional Web Interface)
api:
  enabled: false
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["*"]

# Celery (Optional Task Queue)
celery:
  broker_url: "redis://localhost:6379/0"
  result_backend: "redis://localhost:6379/0"
  task_serializer: "json"
  result_serializer: "json"
  accept_content: ["json"]

